{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xk9TXz6LKvwH",
        "outputId": "523f161e-3fb8-4d43-8d6a-ee63931fe033"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.2)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.16.0)\n",
            "Requirement already satisfied: evaluate in /usr/local/lib/python3.10/dist-packages (0.4.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.15.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (10.0.1)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.15)\n",
            "Requirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.1)\n",
            "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.18.0)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.2.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.5.26)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (16.0.6)\n",
            "Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.5.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.34.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.60.0)\n",
            "Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.1)\n",
            "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n",
            "Requirement already satisfied: keras<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.42.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.11.17)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.5.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.0.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.3.post1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (5.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow) (2.1.3)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.5.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (3.2.2)\n",
            "Requirement already satisfied: transformers[torch] in /usr/local/lib/python3.10/dist-packages (4.35.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.19.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.4.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (4.66.1)\n",
            "Requirement already satisfied: torch!=1.12.0,>=1.10 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.1.0+cu121)\n",
            "Requirement already satisfied: accelerate>=0.20.3 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.25.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.20.3->transformers[torch]) (5.9.5)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers[torch]) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers[torch]) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (3.1.2)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (2.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2023.11.17)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch!=1.12.0,>=1.10->transformers[torch]) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch!=1.12.0,>=1.10->transformers[torch]) (1.3.0)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.25.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (23.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.1.0+cu121)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.19.4)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.1.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (4.66.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2023.11.17)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "%pip install transformers datasets evaluate scikit-learn tensorflow nltk\n",
        "%pip install transformers[torch]\n",
        "%pip install accelerate -U"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Обработка данных"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Сначала загружаем датасет с hugging face и формируем датафрэйм с парами текст-вопрос на котором будет проводиться обучение."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359,
          "referenced_widgets": [
            "0f9bb7019464448f87a5f037c853e899",
            "91b8e01dddf74e32bc8e57bf84f12d2f",
            "b1457ec2891747babe8ad650a7b33fac",
            "0b2767cbe21445c58533ac3562ab5f4c",
            "0b29d1877e1e46f48aec672af5c1cabb",
            "cc3298c74e9440168beacbb18c935531",
            "f304b08aa06a48aea4a9a5fe939d8b54",
            "47745b1b8e7e40b6a6ef2a64068064b6",
            "c614c4d807bc4adeaf3385943938671f",
            "43eb29091d8b44c089ba8842846dc886",
            "7972cd3932a744f883cb8ffc6e5a40b6",
            "ba55ca8b48b74f019cb87763d1eb6731",
            "efa23ef8970f48d78c7ff2e9eecb32f8",
            "09d930f5bcdf408a9670dd03276f6d3e",
            "f22255917e1848d7b45ad906bfdeac18",
            "37c074c28f50433a8ff9e0af1be071ac",
            "4b59b2b05cfe45b299607039a42138b5"
          ]
        },
        "id": "uA4g_eYyQ0o_",
        "outputId": "fe70b9b4-60dc-47a6-951f-9d4171a888db"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0f9bb7019464448f87a5f037c853e899",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zAJrW9n5K3xX"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XXKw4f_uK8Mj"
      },
      "outputs": [],
      "source": [
        "data = pd.DataFrame(load_dataset(\"squad_v2\")[\"train\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Для обучение берем 50% датасета."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b12ZqYM9ir-h"
      },
      "outputs": [],
      "source": [
        "data, dummy = train_test_split(data, train_size=0.5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jSDrWRvsLXF8"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "import random\n",
        "import string\n",
        "import time\n",
        "import pickle\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import optimizers\n",
        "from tensorflow.keras.layers import add\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import Input, Dense, LSTM, GRU, TimeDistributed\n",
        "from tensorflow.keras.layers import Embedding, Dropout, Bidirectional, Concatenate, Lambda\n",
        "\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "\n",
        "np.random.seed(1)\n",
        "random.seed(1)\n",
        "snapshot_folder = '/content/drive/MyDrive/questiongen_weigths/'\n",
        "\n",
        "GRU_units = 256\n",
        "batch_size = 32\n",
        "emb_dim = 50\n",
        "\n",
        "init_lr = 0.0005"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uuZBe9yXqmdr",
        "outputId": "b2267c54-9119-47bc-ba9f-623bd7912a7e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 97,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nltk.download(\"punkt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NUN3SP-1gf5u",
        "outputId": "f0238b8b-2349-446e-95a4-2ff3b5023f89"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G8yLuziNg4lT"
      },
      "outputs": [],
      "source": [
        "DRIVE_DIRECTORY = \"/content/drive/MyDrive/\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Теперь мы токенизируем текст, переводим в строчные буквы, очищаем от цифр, пунктуации и фрмируем в пары текст-вопрос."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "72gNHt-vqEYS"
      },
      "outputs": [],
      "source": [
        "def text_clean(t):\n",
        "\n",
        "    tokenized_text = [\n",
        "        w for w in word_tokenize(t.lower())\n",
        "        if w.isalpha()\n",
        "    ]\n",
        "\n",
        "    return \" \".join(tokenized_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UEWHJyUEoOJn"
      },
      "outputs": [],
      "source": [
        "text_pairs = []\n",
        "start_token = '<startseq>'\n",
        "end_token = '<endseq>'\n",
        "for r in data.iterrows():\n",
        "    answer_start = r[1][\"answers\"][\"answer_start\"][0] if len(r[1][\"answers\"][\"answer_start\"]) > 0 else 0\n",
        "    text_pairs.append((\n",
        "        start_token + \" \" + text_clean(r[1][\"context\"][answer_start:]) + \" \" + end_token,\n",
        "        start_token + \" \" + text_clean(r[1][\"question\"]) + \" \" + end_token\n",
        "    ))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gdnBZ1a9pGJm",
        "outputId": "d8748413-eaa9-4a77-fb9b-23fced20944d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('<startseq> forensic anthropology when analyzing skeletal remains biomedical research and medicine brace has criticized this the practice of forensic anthropologists for using the controversial concept race out of convention when they in fact should be talking about regional ancestry he argues that while forensic anthropologists can determine that a skeletal remain comes from a person with ancestors in a specific region of africa categorizing that skeletal as being black is a socially constructed category that is only meaningful in the particular context of the united states and which is not itself scientifically valid <endseq>',\n",
              "  '<startseq> what type of anthropology is race sometimes still used within <endseq>'),\n",
              " ('<startseq> not hispanic or latino however the practice of separating race and ethnicity as different categories has been criticized both by the american anthropological association and members of commission on civil rights <endseq>',\n",
              "  '<startseq> in addition to hispanic and latino what other ethnic category is included in the united states census <endseq>'),\n",
              " ('<startseq> stephen hawking in particular has addressed a connection between time and the big bang in a brief history of time and elsewhere hawking says that even if time did not begin with the big bang and there were another time frame before the big bang no information from events then would be accessible to us and nothing that happened then would have any effect upon the present upon occasion hawking has stated that time actually began with the big bang and that questions about what happened before the big bang are meaningless this but commonly repeated formulation has received criticisms from philosophers such as aristotelian philosopher mortimer adler <endseq>',\n",
              "  '<startseq> what does stephen hawking make a connection between nothing and <endseq>'),\n",
              " ('<startseq> the incorporation of the first amendment establishment clause in the landmark case of everson board of education has impacted the subsequent interpretation of the separation of church and state in regard to the state governments although upholding the state law in that case which provided for public busing to private religious schools the supreme court held that the first amendment establishment clause was fully applicable to the state governments a more recent case involving the application of this principle against the states was board of education of kiryas joel village school district grumet <endseq>',\n",
              "  '<startseq> what was a past case involving the application of the principle of the establishment clause against states <endseq>'),\n",
              " ('<startseq> modernizing factories houses decline of coal heating and cars and contaminated areas such as the former uranium surface mines around ronneburg have been remediated today environmental problems are the salination of the werra river caused by discharges of salt mines around unterbreizbach and overfertilisation in agriculture damaging the soil and small rivers <endseq>',\n",
              "  '<startseq> what is one thing that helped to improve condition of forests rivers and air <endseq>')]"
            ]
          },
          "execution_count": 102,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "text_pairs[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-9kczcMHgZgG",
        "outputId": "008ac599-2026-44b5-ed8a-5a19bf934810"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "104255"
            ]
          },
          "execution_count": 103,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print(f\"Количество пар вопрос-ответ: {len(text_pairs)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1MIOtH04nGUc",
        "outputId": "da19b00c-d738-45cd-dd7b-720e8a1c7faf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "percentile 80 of len of questions: 122.0\n",
            "longest sentence:  555\n",
            "\n",
            "percentile 80 of len of answers: 14.0\n",
            "longest sentence:  42\n",
            "\n",
            "max-len questions for training:  122\n",
            "max-len answers for training:  14\n"
          ]
        }
      ],
      "source": [
        "def max_length(pairs,prct):\n",
        "    # Create a list of all the captions\n",
        "    questions = []\n",
        "    answers = []\n",
        "    for i in pairs:\n",
        "        questions.append(i[0])\n",
        "        answers.append(i[1])\n",
        "\n",
        "    length_questions = list(len(d.split()) for d in questions)\n",
        "    length_answers = list(len(d.split()) for d in answers)\n",
        "\n",
        "    print('percentile {} of len of questions: {}'.format(prct,np.percentile(length_questions, prct)))\n",
        "    print('longest sentence: ', max(length_questions))\n",
        "    print()\n",
        "    print('percentile {} of len of answers: {}'.format(prct,np.percentile(length_answers, prct)))\n",
        "    print('longest sentence: ', max(length_answers))\n",
        "    print()\n",
        "    return int(np.percentile(length_questions, prct)),int(np.percentile(length_answers, prct))\n",
        "\n",
        "max_len_q,max_len_a = max_length(text_pairs, 80)\n",
        "\n",
        "print('max-len questions for training: ', max_len_q)\n",
        "print('max-len answers for training: ', max_len_a)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0kgrHF9hyJ5J",
        "outputId": "4dcdac2e-876c-4f6a-ba7d-b81f90599769"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "68542"
            ]
          },
          "execution_count": 105,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Remove questions and answers that are shorter than 2 words and longer than maxlen.\n",
        "min_line_len = 2 # two words are for tokens\n",
        "\n",
        "def set_length(tokenized_pairs):\n",
        "    pairs_final = []\n",
        "    for p in tokenized_pairs:\n",
        "        if (\n",
        "            len(p[0].split())>=min_line_len and len(p[1].split())>=min_line_len\n",
        "           and len(p[0].split())<=max_len_q and len(p[1].split())<=max_len_a):\n",
        "\n",
        "            pairs_final.append(p)\n",
        "\n",
        "    return pairs_final\n",
        "\n",
        "pairs_final = set_length(text_pairs)\n",
        "len(pairs_final)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Создаем словарь в который включаются все слова встречающиеся более четырех раз."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Etd3HBWawXOb",
        "outputId": "13ac5b1e-eace-445f-ef17-66826197ae40"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Short vocab size: 58071 \n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "['<endseq>', '<startseq>', 'a', 'aa', 'aaa']"
            ]
          },
          "execution_count": 106,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# making a vocabulary of the words that occur more than word_count_threshold time\n",
        "def create_reoccurring_vocab(pairs, word_count_threshold = 5):\n",
        "    p = pairs\n",
        "    # Create a list of all the captions\n",
        "    all_captions = []\n",
        "    for i in p:\n",
        "        for j in i:\n",
        "            all_captions.append(j)\n",
        "\n",
        "    # Consider only words which occur at least 10 times in the corpus\n",
        "    word_counts = {}\n",
        "    nsents = 0\n",
        "    for sent in all_captions:\n",
        "        nsents += 1\n",
        "        for w in sent.split(' '):\n",
        "            word_counts[w] = word_counts.get(w, 0) + 1\n",
        "\n",
        "    vocab = [w for w in word_counts if word_counts[w] >= word_count_threshold]\n",
        "    vocab = list(set(vocab))\n",
        "    print('Short vocab size: %d ' % len(vocab))\n",
        "    return vocab\n",
        "\n",
        "short_vocab = create_reoccurring_vocab(text_pairs, word_count_threshold = 4)\n",
        "\n",
        "short_vocab = sorted(short_vocab)[1:]\n",
        "short_vocab[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xPz-0nyFwaig",
        "outputId": "4fccb975-f299-4f05-ac90-a2c4f706229c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "58071"
            ]
          },
          "execution_count": 107,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "vocab_len = len(short_vocab) + 1 # since index 0 is used as padding, we have to increase the vocab size\n",
        "vocab_len"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eiz8yzClxLTA"
      },
      "outputs": [],
      "source": [
        "import pickle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Отсекаем пары которые содеражат редкие слова не встречающиеся в нашем словаре."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 366
        },
        "id": "Ed5qxGVoxpfI",
        "outputId": "035d1169-662e-4e74-dff1-2b9c9345e830"
      },
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-109-ebd4d1985670>\u001b[0m in \u001b[0;36m<cell line: 32>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;31m# # Trim voc and pairs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m \u001b[0mpairs_final\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrimRareWords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshort_vocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpairs_final\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'final_pairs_v21.pkl'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0mpairs_final\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpairs_final\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-109-ebd4d1985670>\u001b[0m in \u001b[0;36mtrimRareWords\u001b[0;34m(voc, pairs)\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;31m# Check input sentence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minput_sentence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvoc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m                 \u001b[0mkeep_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# keep the pairs that have the words in vocab\n",
        "def trimRareWords(voc, pairs):\n",
        "    # Filter out pairs with trimmed words\n",
        "    keep_pairs = []\n",
        "    i=0\n",
        "    for pair in pairs:\n",
        "        i+=1\n",
        "        input_sentence = pair[0]\n",
        "        output_sentence = pair[1]\n",
        "        keep_input = True\n",
        "        keep_output = True\n",
        "        # Check input sentence\n",
        "        for word in input_sentence.split(' '):\n",
        "            if word not in voc:\n",
        "                keep_input = False\n",
        "                break\n",
        "        # Check output sentence\n",
        "        for word in output_sentence.split(' '):\n",
        "            if word not in voc:\n",
        "                keep_output = False\n",
        "                break\n",
        "\n",
        "        # Only keep pairs that do not contain trimmed word(s) in their input or output sentence\n",
        "        if keep_input and keep_output:\n",
        "            keep_pairs.append(pair)\n",
        "\n",
        "    print(\"\\nTrimmed from {} pairs to {}\".format(len(pairs), len(keep_pairs)))\n",
        "    return keep_pairs\n",
        "\n",
        "\n",
        "# # Trim voc and pairs\n",
        "pairs_final = trimRareWords(short_vocab, pairs_final)\n",
        "with open ('final_pairs_v21.pkl','wb') as f:\n",
        "    pairs_final = pickle.dump(pairs_final,f)\n",
        "\n",
        "with open ('final_pairs_v21.pkl','rb') as f:\n",
        "    pairs_final = pickle.load(f)\n",
        "\n",
        "pairs_final_train = pairs_final\n",
        "len(pairs_final_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Задание нейронной сети"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PDyE07YyzfPt"
      },
      "outputs": [],
      "source": [
        "#Create an instance of the tokenizer object:\n",
        "tokenizer = Tokenizer(filters = [])\n",
        "tokenizer.fit_on_texts(short_vocab)\n",
        "\n",
        "ixtoword = {} # index to word dic\n",
        "wordtoix = tokenizer.word_index # word to index dic\n",
        "pad_token = 'pad0'\n",
        "ixtoword[0] = pad_token # no word in vocab has index 0. but padding is indicated with 0\n",
        "\n",
        "for w in tokenizer.word_index:\n",
        "    ixtoword[tokenizer.word_index[w]] = w"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zuG7zcRXzh_G"
      },
      "outputs": [],
      "source": [
        "# Making the embedding matrix\n",
        "def make_embedding_layer(embedding_dim=50, glove=True):\n",
        "    print('Loading glove...')\n",
        "    glove_dir = DRIVE_DIRECTORY\n",
        "    embeddings_index = {}\n",
        "    f = open(os.path.join(glove_dir, 'glove.6B.'+str(embedding_dim)+'d.txt'), encoding=\"utf-8\")\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        coefs = np.asarray(values[1:], dtype='float32')\n",
        "        embeddings_index[word] = coefs\n",
        "    f.close()\n",
        "    print(\"GloVe \",embedding_dim, ' loaded.')\n",
        "    # Get 200-dim dense vector for each of the vocab_rocc\n",
        "    embedding_matrix = np.zeros((vocab_len, embedding_dim)) # to import as weights for Keras Embedding layer\n",
        "    for word, i in wordtoix.items():\n",
        "        embedding_vector = embeddings_index.get(word)\n",
        "        if embedding_vector is not None:\n",
        "            # Words not found in the embedding index will be all zeros\n",
        "            embedding_matrix[i] = embedding_vector\n",
        "\n",
        "    embedding_layer = Embedding(vocab_len, embedding_dim, mask_zero=True, trainable=False) # we have a limited vocab so we\n",
        "                                                                                           # do not train the embedding layer\n",
        "                                                                                           # we use 0 as padding so => mask_zero=True\n",
        "    embedding_layer.build((None,))\n",
        "    embedding_layer.set_weights([embedding_matrix])\n",
        "\n",
        "    return embedding_layer\n",
        "\n",
        "embeddings = make_embedding_layer(embedding_dim=emb_dim)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "--bxAVNXzkMQ"
      },
      "outputs": [],
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "    def __init__(self, embedding_dim, enc_units):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.batch_sz = batch_size\n",
        "        self.enc_units = enc_units\n",
        "        self.embeddings = embeddings\n",
        "\n",
        "        self.Bidirectional1 = Bidirectional(GRU(enc_units,\n",
        "                                                return_sequences=True,\n",
        "                                                return_state=True,\n",
        "                                                recurrent_initializer='glorot_uniform',\n",
        "                                                name='gru_1'), name='bidirectional_encoder1')\n",
        "\n",
        "        self.dropout = Dropout(0.2)\n",
        "        self.Inp = Input(shape=(max_len_q,)) # size of questions\n",
        "\n",
        "    def call(self, x, hidden):\n",
        "        x = self.embeddings(x)\n",
        "        x = self.dropout(x)\n",
        "        output, state_f, state_b = self.Bidirectional1(x)\n",
        "\n",
        "        return output, state_f, state_b\n",
        "\n",
        "    def initialize_hidden_state(self):\n",
        "        return tf.zeros((self.batch_sz, self.enc_units))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S6HXKNrm314Z"
      },
      "outputs": [],
      "source": [
        "encoder = Encoder(emb_dim, GRU_units)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3YXXBLj433Qv"
      },
      "outputs": [],
      "source": [
        "class BahdanauAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, units):\n",
        "        super(BahdanauAttention, self).__init__()\n",
        "        self.W1 = tf.keras.layers.Dense(units)\n",
        "        self.W2 = tf.keras.layers.Dense(units)\n",
        "        self.V = tf.keras.layers.Dense(1)\n",
        "        self.units = units\n",
        "\n",
        "    def call(self, query, values):\n",
        "\n",
        "        # query hidden state shape == (batch_size, hidden size)\n",
        "        # query_with_time_axis shape == (batch_size, 1, hidden size)\n",
        "        # values shape == (batch_size, max_len, hidden size)\n",
        "        # we are doing this to broadcast addition along the time axis to calculate the score\n",
        "        query_with_time_axis = tf.expand_dims(query, 1)\n",
        "\n",
        "        # score shape == (batch_size, max_length, 1)\n",
        "        # we get 1 at the last axis because we are applying score to self.V\n",
        "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
        "        score = self.V(tf.nn.tanh(\n",
        "            self.W1(query_with_time_axis) + self.W2(values)))\n",
        "\n",
        "        # attention_weights shape == (batch_size, max_length, 1)\n",
        "        attention_weights = tf.nn.softmax(score, axis=1)\n",
        "\n",
        "        # context_vector shape after sum == (batch_size, hidden_size)\n",
        "        context_vector = attention_weights * values\n",
        "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "\n",
        "        return context_vector, attention_weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fIp_IoAH341p"
      },
      "outputs": [],
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_dim, dec_units):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.batch_sz = batch_size\n",
        "        self.embeddings = embeddings\n",
        "        self.units = 2 * dec_units # because we use bidirectional encoder\n",
        "        self.fc = Dense(vocab_len, activation='softmax', name='dense_layer')\n",
        "        # used for attention\n",
        "        self.attention = BahdanauAttention(self.units)\n",
        "        self.decoder_gru_l1 = GRU(self.units, return_sequences=True,\n",
        "                                  return_state= False, recurrent_initializer='glorot_uniform' ,name='decoder_gru1')\n",
        "        self.decoder_gru_l2 = GRU(self.units, return_sequences=False,\n",
        "                                  return_state= True, recurrent_initializer='glorot_uniform' ,name='decoder_gru2')\n",
        "        self.dropout = Dropout(0.4)\n",
        "\n",
        "    def call(self, x, hidden, enc_output):\n",
        "\n",
        "        # enc_output shape == (batch_size, max_length, hidden_size)\n",
        "        context_vector, attention_weights = self.attention(hidden, enc_output)\n",
        "\n",
        "        # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
        "        x = self.embeddings(x)\n",
        "\n",
        "        # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
        "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1) # concat input and context vector together\n",
        "\n",
        "        # passing the concatenated vector to the GRU\n",
        "        x = self.decoder_gru_l1(x)\n",
        "        x = self.dropout(x)\n",
        "        output, state = self.decoder_gru_l2(x)\n",
        "        x = self.fc(output)\n",
        "        return x, state, attention_weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s2AN9_AI37ML"
      },
      "outputs": [],
      "source": [
        "decoder = Decoder(vocab_len, emb_dim, GRU_units)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4YVNhbjc38W8"
      },
      "outputs": [],
      "source": [
        "import unicodedata\n",
        "import matplotlib.ticker as ticker\n",
        "\n",
        "def unicode_to_ascii(s):\n",
        "    return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
        "      if unicodedata.category(c) != 'Mn')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JSri6-Qj39my"
      },
      "outputs": [],
      "source": [
        "def evaluate(sentence):\n",
        "\n",
        "    attention_plot = np.zeros((max_len_a, max_len_q))\n",
        "\n",
        "    sentence = unicode_to_ascii(sentence.lower())\n",
        "    inputs = [wordtoix[i] for i in sentence.split(' ')]\n",
        "    inputs = [wordtoix[start_token]]+inputs+[wordtoix[end_token]]\n",
        "    inputs = pad_sequences([inputs],maxlen=max_len_q, padding='post')\n",
        "\n",
        "    inputs = tf.convert_to_tensor(inputs)\n",
        "\n",
        "    result = ''\n",
        "\n",
        "    hidden = [tf.zeros((1, GRU_units))]\n",
        "    enc_out, enc_hidden_f, enc_hidden_b = encoder(inputs, hidden)\n",
        "\n",
        "    dec_hidden = Concatenate(axis=-1)([enc_hidden_f, enc_hidden_b])\n",
        "    dec_input = tf.expand_dims([wordtoix[start_token]], 1)\n",
        "\n",
        "    for t in range(max_len_a):\n",
        "        predictions, dec_hidden, attention_weights = decoder(dec_input,\n",
        "                                                             dec_hidden,\n",
        "                                                             enc_out)\n",
        "\n",
        "        attention_weights = tf.reshape(attention_weights, (-1, ))\n",
        "        attention_plot[t] = K.get_value(attention_weights)\n",
        "\n",
        "        predicted_id =  K.get_value(tf.argmax(predictions[0]))\n",
        "\n",
        "        if ixtoword[predicted_id] == end_token:\n",
        "            return result, sentence, attention_plot\n",
        "\n",
        "        result += ixtoword[predicted_id] + ' '\n",
        "        # the predicted ID is fed back into the model\n",
        "        dec_input = tf.expand_dims([predicted_id], 1)\n",
        "\n",
        "    return result, sentence, attention_plot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cYZ5aq3Q3_QB"
      },
      "outputs": [],
      "source": [
        "def answer(sentence, training=False):\n",
        "    result, sentence, attention_plot = evaluate(sentence)\n",
        "\n",
        "    if training:\n",
        "        return result\n",
        "\n",
        "    print('Input: %s' % (sentence))\n",
        "    print('Predicted answer: {}'.format(result))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EZXAe2If4BkM"
      },
      "outputs": [],
      "source": [
        "def beam_search(sentence, k=3, maxsample=max_len_a, use_unk=False, oov=None, eos=end_token):\n",
        "    \"\"\"return k samples (beams) and their NLL scores, each sample is a sequence of labels,\n",
        "    all samples starts with an `empty` label and end with `eos` or truncated to length of `maxsample`.\n",
        "    You need to supply `predict` which returns the label probability of each sample.\n",
        "    `use_unk` allow usage of `oov` (out-of-vocabulary) label in samples\n",
        "    \"\"\"\n",
        "\n",
        "    dead_k = 0 # samples that reached eos\n",
        "    dead_samples = []\n",
        "    dead_scores = []\n",
        "    live_k = 1 # samples that did not yet reached eos\n",
        "    live_samples = [[wordtoix[start_token]]]\n",
        "    live_scores = [0]\n",
        "\n",
        "    sentence = unicode_to_ascii(sentence.lower())\n",
        "    inputs = [wordtoix[i] for i in sentence.split(' ')]\n",
        "    inputs = [wordtoix[start_token]]+inputs+[wordtoix[end_token]]\n",
        "    inputs = pad_sequences([inputs],maxlen=max_len_q, padding='post')\n",
        "\n",
        "    inputs = tf.convert_to_tensor(inputs)\n",
        "\n",
        "    hidden = [tf.zeros((1, GRU_units))]\n",
        "    enc_out, enc_hidden_f, enc_hidden_b = encoder(inputs, hidden)\n",
        "\n",
        "    dec_hidden = Concatenate(axis=-1)([enc_hidden_f, enc_hidden_b])\n",
        "    dec_input = tf.expand_dims([wordtoix[start_token]], 0)\n",
        "\n",
        "    while live_k and dead_k < k:\n",
        "        # for every possible live sample calc prob for every possible label\n",
        "        predictions, dec_hidden, _ = decoder(dec_input,dec_hidden,enc_out)\n",
        "        probs = K.get_value(predictions[0])\n",
        "        # total score for every sample is sum of -log of word prb\n",
        "        cand_scores = np.array(live_scores)[:,None] - np.log(probs)\n",
        "        if not use_unk and oov is not None:\n",
        "            cand_scores[:,oov] = 1e20\n",
        "        cand_flat = cand_scores.flatten()\n",
        "\n",
        "        # find the best (lowest) scores we have from all possible samples and new words\n",
        "        ranks_flat = cand_flat.argsort()[:(k-dead_k)]\n",
        "        live_scores = cand_flat[ranks_flat]\n",
        "\n",
        "        # append the new words to their appropriate live sample\n",
        "        voc_size = vocab_len\n",
        "        live_samples = [live_samples[r//voc_size]+[r%voc_size] for r in ranks_flat]\n",
        "\n",
        "        # live samples that should be dead are...\n",
        "        zombie = [s[-1] == eos or len(s) >= maxsample for s in live_samples]\n",
        "\n",
        "        # add zombies to the dead\n",
        "        dead_samples += [s for s,z in zip(live_samples,zombie) if z]  # remove first label == empty\n",
        "        dead_scores += [s for s,z in zip(live_scores,zombie) if z]\n",
        "        dead_k = len(dead_samples)\n",
        "        # remove zombies from the living\n",
        "        live_samples = [s for s,z in zip(live_samples,zombie) if not z]\n",
        "        live_scores = [s for s,z in zip(live_scores,zombie) if not z]\n",
        "        live_k = len(live_samples)\n",
        "\n",
        "    final_samples = dead_samples + live_samples\n",
        "    final_scores = dead_scores + live_scores\n",
        "\n",
        "    # cutting the strong where end_token is encounterd\n",
        "    for i in range(len(final_scores)):\n",
        "        final_scores[i] /= len(final_samples[i]) # normalizing the scores\n",
        "\n",
        "    final_result =[]\n",
        "\n",
        "    for i in range(len(final_scores)):\n",
        "        final_result.append((final_scores[i],final_samples[i]))\n",
        "\n",
        "    final_list_ix = max(final_result)[1]\n",
        "    final_list_word = [ixtoword[f] for f in final_list_ix]\n",
        "    final_sentence = ' '.join(final_list_word[1:])\n",
        "    end_ix = final_sentence.find(end_token)\n",
        "    return final_sentence[:end_ix]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JHknoa4j4DZ-"
      },
      "outputs": [],
      "source": [
        "optimizer = tf.keras.optimizers.Adam(init_lr)\n",
        "\n",
        "def loss_function(real, pred):\n",
        "\n",
        "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "    loss_ = K.sparse_categorical_crossentropy(real, pred, from_logits= False)\n",
        "\n",
        "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "    loss_ *= mask\n",
        "\n",
        "    return tf.reduce_mean(loss_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "30MoAnz34FRL"
      },
      "outputs": [],
      "source": [
        "checkpoint_prefix = os.path.join(snapshot_folder, str(emb_dim)+\"-ckpt\")\n",
        "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
        "                                 encoder=encoder,\n",
        "                                 decoder=decoder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C3351S6B4ICy"
      },
      "outputs": [],
      "source": [
        "@tf.function\n",
        "def train_step(inp, targ, enc_hidden):\n",
        "    loss = 0\n",
        "    with tf.GradientTape() as tape:\n",
        "        enc_output, enc_hidden_f, enc_hidden_b = encoder(inp, enc_hidden)\n",
        "\n",
        "        dec_hidden = Concatenate(axis=-1)([enc_hidden_f, enc_hidden_b])\n",
        "        dec_input = tf.expand_dims([wordtoix[start_token]] * batch_size, 1) # dec_input initially == start_token\n",
        "\n",
        "        # Teacher forcing - feeding the target as the next input\n",
        "        for t in range(1, targ.shape[1]):\n",
        "\n",
        "            # passing enc_output to the decoder\n",
        "            predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
        "            loss += loss_function(targ[:, t], predictions) # each time just use one timestep output\n",
        "            # using teacher forcing\n",
        "            dec_input = tf.expand_dims(targ[:, t], 1) # expected output at this time becomes input for next timestep\n",
        "\n",
        "    batch_loss = (loss / int(targ.shape[1]))\n",
        "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "    gradients = tape.gradient(loss, variables)\n",
        "    optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "    return batch_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5vNPuwXG4Jr1"
      },
      "outputs": [],
      "source": [
        "history={'loss':[]}\n",
        "smallest_loss = np.inf\n",
        "best_ep = 1\n",
        "EPOCHS = 157\n",
        "enc_hidden = encoder.initialize_hidden_state()\n",
        "steps_per_epoch = len(pairs_final_train)//batch_size # used for caculating number of batches\n",
        "current_ep = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kfDGw_2Z4LC3"
      },
      "outputs": [],
      "source": [
        "def plot_history():\n",
        "    plt.figure(figsize=(4,3))\n",
        "    plt.plot(best_ep,smallest_loss,'ro')\n",
        "    plt.plot(history['loss'],'b-')\n",
        "    plt.legend(['best','loss'])\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Собственно обучение"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WEcnGKDS4NW9"
      },
      "outputs": [],
      "source": [
        "batch_loss = K.constant(0)\n",
        "X, y = [], []\n",
        "for ep in range(current_ep, EPOCHS+1):\n",
        "    current_ep = ep\n",
        "    start = time.time()\n",
        "    total_loss = 0\n",
        "    btch = 1\n",
        "\n",
        "    for p in pairs_final_train:\n",
        "\n",
        "        question = p[0]\n",
        "        label = p[1]\n",
        "        # find the index of each word of the caption in vocabulary\n",
        "        question_seq = [wordtoix[word] for word in question.split(' ') if word in wordtoix]\n",
        "        label_seq = [wordtoix[word] for word in label.split(' ') if word in wordtoix]\n",
        "        # encoder input and decoder input and label\n",
        "        enc_in_seq = pad_sequences([question_seq], maxlen=max_len_q, padding='post')[0]\n",
        "        dec_out_seq = pad_sequences([label_seq], maxlen=max_len_a, padding='post')[0]\n",
        "\n",
        "        X.append(enc_in_seq)\n",
        "        y.append(dec_out_seq)\n",
        "\n",
        "        if len(X) == batch_size :\n",
        "            batch_loss = train_step(np.array(X), np.array(y), enc_hidden)\n",
        "            total_loss += batch_loss\n",
        "            X , y = [], []\n",
        "            btch += 1\n",
        "            if btch % (steps_per_epoch//6) == 0:\n",
        "                print('Epoch {} Batch {} Loss: {:.4f}'.format(ep , btch, K.get_value(batch_loss)))\n",
        "\n",
        "    epoch_loss =  K.get_value(total_loss) / steps_per_epoch\n",
        "    print('\\n*** Epoch {} Loss {:.4f} ***\\n'.format(ep ,epoch_loss))\n",
        "    history['loss'].append(epoch_loss)\n",
        "\n",
        "    checkpoint.save(file_prefix = checkpoint_prefix)\n",
        "\n",
        "    if epoch_loss < smallest_loss:\n",
        "        smallest_loss = epoch_loss\n",
        "        best_ep = ep\n",
        "        print('check point saved!')\n",
        "\n",
        "    if ep % 3 == 0:\n",
        "        plot_history()\n",
        "\n",
        "    print('Best epoch so far: ',best_ep,' smallest loss:',smallest_loss)\n",
        "    print('Time taken for the epoch {:.3f} sec\\n'.format(time.time() - start))\n",
        "\n",
        "    print('=' * 40)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Тесты"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JzcCBcKq_t_q"
      },
      "outputs": [],
      "source": [
        "answer(\"North Pyramid is the largest of the pyramids located in Cairo Egypt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "celtHpjhSIY5"
      },
      "outputs": [],
      "source": [
        "answer(\"Thomas II lived in Hungary\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VQDCdPMJmXz8"
      },
      "outputs": [],
      "source": [
        "answer(\"Atop the Main Building gold dome is a golden statue of the Virgin Mary\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "09d930f5bcdf408a9670dd03276f6d3e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0b2767cbe21445c58533ac3562ab5f4c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "CheckboxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "CheckboxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "CheckboxView",
            "description": "Add token as git credential?",
            "description_tooltip": null,
            "disabled": false,
            "indent": true,
            "layout": "IPY_MODEL_ba55ca8b48b74f019cb87763d1eb6731",
            "style": "IPY_MODEL_efa23ef8970f48d78c7ff2e9eecb32f8",
            "value": true
          }
        },
        "0b29d1877e1e46f48aec672af5c1cabb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ButtonModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Login",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_09d930f5bcdf408a9670dd03276f6d3e",
            "style": "IPY_MODEL_f22255917e1848d7b45ad906bfdeac18",
            "tooltip": ""
          }
        },
        "0f9bb7019464448f87a5f037c853e899": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "VBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_91b8e01dddf74e32bc8e57bf84f12d2f",
              "IPY_MODEL_b1457ec2891747babe8ad650a7b33fac",
              "IPY_MODEL_0b2767cbe21445c58533ac3562ab5f4c",
              "IPY_MODEL_0b29d1877e1e46f48aec672af5c1cabb",
              "IPY_MODEL_cc3298c74e9440168beacbb18c935531"
            ],
            "layout": "IPY_MODEL_f304b08aa06a48aea4a9a5fe939d8b54"
          }
        },
        "37c074c28f50433a8ff9e0af1be071ac": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "43eb29091d8b44c089ba8842846dc886": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "47745b1b8e7e40b6a6ef2a64068064b6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4b59b2b05cfe45b299607039a42138b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7972cd3932a744f883cb8ffc6e5a40b6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "91b8e01dddf74e32bc8e57bf84f12d2f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_47745b1b8e7e40b6a6ef2a64068064b6",
            "placeholder": "​",
            "style": "IPY_MODEL_c614c4d807bc4adeaf3385943938671f",
            "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"
          }
        },
        "b1457ec2891747babe8ad650a7b33fac": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "PasswordModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "PasswordModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "PasswordView",
            "continuous_update": true,
            "description": "Token:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_43eb29091d8b44c089ba8842846dc886",
            "placeholder": "​",
            "style": "IPY_MODEL_7972cd3932a744f883cb8ffc6e5a40b6",
            "value": ""
          }
        },
        "ba55ca8b48b74f019cb87763d1eb6731": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c614c4d807bc4adeaf3385943938671f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cc3298c74e9440168beacbb18c935531": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_37c074c28f50433a8ff9e0af1be071ac",
            "placeholder": "​",
            "style": "IPY_MODEL_4b59b2b05cfe45b299607039a42138b5",
            "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"
          }
        },
        "efa23ef8970f48d78c7ff2e9eecb32f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f22255917e1848d7b45ad906bfdeac18": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ButtonStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "f304b08aa06a48aea4a9a5fe939d8b54": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
